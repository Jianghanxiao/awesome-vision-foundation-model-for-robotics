# ğŸš€ **Awesome Vision Foundation Models for Robotics**
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> âœ¨ A curated list of **Vision Foundation Models (VFMs)** that empower **robot learning** and **robot perception** with strong **zero-shot generalization**.

---

## ğŸ¯ **Why This List?**
Robotics research keeps evolving â€” and so do vision foundation models trained on **huge datasets** with **broad generalization** skills.  
Butâ€¦ ğŸ¤¯ finding the right model for **your specific robotic task** is hard.

So this repo serves as:

âœ… A **dictionary** of VFMs for robotics  
âœ… A **quick lookup** for papers & code  
âœ… A **task-driven map** for integration into pipelines  
âœ… A community-driven reference to stay **up-to-date**

---

## ğŸ§© **What Counts as a Vision Foundation Model?**
A model that:

- âœ… Supports **vision tasks**
- âœ… Shows **zero-shot / few-shot** generalization
- âœ… Has **practical value** for robot perception & interaction ğŸ¤–

> If there is **no code** â†’ ğŸš« **not included**.

> ğŸ’¡ Broad definition: If it advances **robot vision** with **practical benefit**, itâ€™s welcome here.

---

## ğŸ“š **Taxonomy of VFMs in Robotics**

| Category | Example Capabilities | Typical Robotics Use |
|---------|---------------------|---------------------|
| ğŸ§± Visual Latent Representation | Semantic embeddings, open-vocab features | Object understanding, task instruction grounding |
| ğŸ” Detection | object detection | Target localization, open-set recognition |
| âœ‚ï¸ Segmentation | Instance segmentation, part segmentation, universal mask prediction | Grasping, affordance extraction, occlusion handling |
| ğŸ”— Correspondence | Keypoint matching, patch/feature correspondences | Multi-view alignment, object registration |
| ğŸ§­ Tracking | Object/point trajectory, video mask tracking | Visual servoing, dynamic obstacle avoidance, motion prediction |
| ğŸŒ€ Depth Estimation | Monocular/multi-view depth, metric-free or metric depth | Navigation, coarse obstacle avoidance, 3D scene understanding |
| ğŸ—ï¸ 3D Reconstruction | Point cloud / mesh generation from sparse views or streaming  | Digital twins, SLAM, simulation pipelines |

> ğŸ”¨ Entries will include: **paper + code link + robotics applicability**

### ğŸŒŒ 2D/3D Generation & VLMs (Recommended Awesome Lists)
These are outside the current scope of VFMs for robotics, but useful references (in the future, we can try to incorporate the useful ones for robotics, welcome contributions from the community):

* [Awesome Text-to-Image](https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image)  
* [Awesome Video Diffusion](https://github.com/showlab/Awesome-Video-Diffusion)  
* [Awesome 3D Generation](https://github.com/justimyhxu/awesome-3D-generation)  
* [Awesome 3D Diffusion](https://github.com/cwchenwang/awesome-3d-diffusion)  
* [Awesome VLLMs](https://github.com/JackYFL/awesome-VLLMs)  

---

## ğŸ¤ **Contributions**
PRs and suggestions are highly welcome! ğŸš€

âœ… Add new papers with usable code  
âœ… Suggest new categories  
âœ… Improve robotics applicability notes
âœ… Include Input / Output / Visualization for each model

Please use this template when adding:
```
### Model/Paper Name â€” [Year]
* Paper: link
* Code: github-repo-link [must have code]
* Input: image / video / text / RGBD / multi-view / prompts / ...
* Output: boxes / masks / embeddings / depth / point cloud / ...
* Visualization: one example figure illustrating input and output
* Robotics applicability: 1â€“2 lines (best to have)
* Integration notes: any requirement or typical use
```

ğŸ“© Letâ€™s build the **go-to hub** for VFMs in robotics.

---

If youâ€™re working on **robot vision** or **robot learning**,  
â­ **Star** this repo & help the community grow!

---
---
---
## ğŸ§± Visual Latent Representation

## ğŸ” Detection

## âœ‚ï¸ Segmentation


## ğŸ”— Correspondence 

## ğŸ§­ Tracking

## ğŸŒ€ Depth Estimation

## ğŸ—ï¸ 3D Reconstruction 